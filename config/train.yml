# ------------------------------------------------------------------------------

# config file for training proteusAI

# ------------------------------------------------------------------------------

# Path to the YAML config file. defined here for clarity, but need to call this 
# as CL arg for it to work
config: "config/train.yml" 

# ------------------------------------------------------------------------------

# hyperparameters
hyper_parameters:
  d_model: 256
  d_diffusion: 512
  d_latent: 16
  top_k: 32
  voxel_dims: 8
  cell_dim: 1.0
  sc_vae_layers: 1
  bb_vae_layers: 1
  diff_layers: 1
  diff_parameterization: "x0" # eps, x0, or vpred

# ------------------------------------------------------------------------------

# training params
training_parameters:
  use_amp: False
  epochs: 1000  # number of epochs, training until convergence
  train_type: "diffusion" # sc_vae, bb_vae or diffusion are the options
  checkpoint: 
    path: "" # relative path from output dir (see)
    model_checkpoints: 10 # number of epochs to save a checkpoint of the model after
    sc_vae: True
    bb_vae: True
    diff: False
    adam: False
    sched: False
  inference:
    temperature: !!float 1e-6
  early_stopping:
    thresh: 0.00 # delta validation sequence similarity, if below this value, training is stopped. negative values mean the seq sim must decrease before stopping
    tolerance: 50 # how many epochs to consider when calculating delta seq sim for early stopping. takes the max delta seq sim between current epoch and last n epochs, and decides based on this
  adam:
    beta1: 0.90  # decay rate of momentum term
    beta2: 0.98  # decay rate of variance term
    epsilon: !!float 10e-9  # for numerical stability in param updates (!!float lets PyYAML know this is a float, not str)
    weight_decay: 0.00 # weight decay, set to 0 for no weightdecay, ie regular adam
  regularization:
    dropout: 0.00  # percentage of dropout
    noise_coords_std: 0.00 # stdev of noise injection into coordinates during training
    homo_thresh: 0.70
  loss:
    accumulation_steps: 1
    grad_clip_norm: 1.0 # max L2 norm of gradients for gradient clipping. if set to 0, no gradient clipping is applied (not recommended since the loss is a sum)
    label_smoothing: 0.00
    beta: !!float 1e-3
  lr:
    lr_type: "static"
    lr_step: !!float 1e-4 # max lr, ramps up to this val before decreasing  
    warmup_steps: 4000 # number of warmup steps before decreasing

# ------------------------------------------------------------------------------

# dataset configs
data:
  num_train: -1  # number of training samples to use; -1 means all available
  num_val: -1  # number of validation samples to use; -1 means all available
  num_test: -1  # number of test samples to use; -1 means all available
  min_seq_size: 16 # minimum sequence length, shorter is excluded
  max_seq_size: 8192 # max sequence lengths, longer samples not included
  batch_tokens: 8192 # number of valid tokens per batch (including non-representative chains, basically the tokens used in computation, not necessarily in loss)
  max_resolution: 3.5 # max_resolution of PDBs

# ------------------------------------------------------------------------------

# Output
output:

# ------------------------------------------------------------------------------
